{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CNz35ia6Bz3"
   },
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkRbhMJH6Bz3"
   },
   "source": [
    "### Business Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PBm5xaj6Bz3"
   },
   "source": [
    "The healthcare industry is rapidly evolving, with professionals facing increasing challenges in managing vast volumes of medical data while delivering accurate and timely diagnoses. The need for quick access to comprehensive, reliable, and up-to-date medical knowledge is critical for improving patient outcomes and ensuring informed decision-making in a fast-paced environment.\n",
    "\n",
    "Healthcare professionals often encounter information overload, struggling to sift through extensive research and data to create accurate diagnoses and treatment plans. This challenge is amplified by the need for efficiency, particularly in emergencies, where time-sensitive decisions are vital. Furthermore, access to trusted, current medical information from renowned manuals and research papers is essential for maintaining high standards of care.\n",
    "\n",
    "To address these challenges, healthcare centers can focus on integrating systems that streamline access to medical knowledge, provide tools to support quick decision-making, and enhance efficiency. Leveraging centralized knowledge platforms and ensuring healthcare providers have continuous access to reliable resources can significantly improve patient care and operational effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xDPsqvO6Bz5"
   },
   "source": [
    "**Common Questions to Answer**\n",
    "\n",
    "1. **Critical Care Protocols:** \"What is the protocol for managing sepsis in a critical care unit?\"\n",
    "\n",
    "2. **General Surgery:** \"What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\"\n",
    "\n",
    "3. **Dermatology:** \"What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\"\n",
    "\n",
    "4. **Neurology:** \"What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CARPKFwm6Bz4"
   },
   "source": [
    "### Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOElOEXq6Bz4"
   },
   "source": [
    "As an AI specialist, your task is to develop a RAG-based AI solution using renowned medical manuals to address healthcare challenges. The objective is to **understand** issues like information overload, **apply** AI techniques to streamline decision-making, **analyze** its impact on diagnostics and patient outcomes, **evaluate** its potential to standardize care practices, and **create** a functional prototype demonstrating its feasibility and effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "by9EvAnkSpZf"
   },
   "source": [
    "### Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jw5LievCSru2"
   },
   "source": [
    "The **Merck Manuals** are medical references published by the American pharmaceutical company Merck & Co., that cover a wide range of medical topics, including disorders, tests, diagnoses, and drugs. The manuals have been published since 1899, when Merck & Co. was still a subsidiary of the German company Merck.\n",
    "\n",
    "The manual is provided as a PDF with over 4,000 pages divided into 23 sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnwETBOE6Bz5"
   },
   "source": [
    "## Installing and Importing Necessary Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIioYWySiwMf"
   },
   "source": [
    "This command installs all the necessary Python libraries required to build and evaluate a Retrieval-Augmented Generation (RAG) pipeline.\n",
    "langchain and langchain_community provide tools to build LLM-based applications and manage components like document loaders, retrievers, and chains.\n",
    "chromadb is a vector database used for storing and retrieving embeddings efficiently.\n",
    "pymupdf enables reading and parsing PDF documents for text extraction.\n",
    "tiktoken is used for tokenizing text compatible with OpenAI models, essential for chunking and embedding.\n",
    "datasets from Hugging Face provides utilities for loading and managing datasets used during evaluation.\n",
    "evaluate offers evaluation metrics such as accuracy and BLEU for assessing model performance.\n",
    "langchain_openai integrates OpenAI models with LangChain for embedding generation and response synthesis.\n",
    "ragas (Retrieval-Augmented Generation Assessment) is used to evaluate RAG systems using metrics like faithfulness, context precision, and answer relevancy.\n",
    "The -q flag ensures the installation process runs quietly without verbose output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36431,
     "status": "ok",
     "timestamp": 1762623392214,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "0VOckDVkWGei",
    "outputId": "9e94766f-0ba1-4ce6-9332-60ee6d20f17b"
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install -q langchain_community==0.3.27 \\\n",
    "              langchain==0.3.27 \\\n",
    "              chromadb==1.0.15 \\\n",
    "              pymupdf==1.26.3 \\\n",
    "              tiktoken==0.9.0 \\\n",
    "              datasets==4.0.0 \\\n",
    "              evaluate==0.4.5 \\\n",
    "              langchain_openai==0.3.30 \\\n",
    "              ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xq5UfxEhtMHe"
   },
   "source": [
    "**Installing required dependencies**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYI1Id5OjOXj"
   },
   "source": [
    "This section imports all the essential libraries required to build, process, and evaluate a Retrieval-Augmented Generation (RAG) pipeline.\n",
    "\n",
    "The os module allows interaction with the operating system, such as setting environment variables, and json handles reading and writing JSON data.\n",
    "PyMuPDFLoader from langchain.document_loaders is used to load and extract text content from PDF files, while OpenAI provides access to OpenAI’s models and services.\n",
    "\n",
    "For data processing, tiktoken handles tokenization—counting and splitting text into manageable pieces for language models—and pandas supports loading and analyzing tabular data.\n",
    "\n",
    "LangChain components such as RecursiveCharacterTextSplitter break text into overlapping chunks for embedding, OpenAIEmbeddings generates vector representations of text using OpenAI models, and Chroma stores and retrieves these embeddings for semantic search.\n",
    "\n",
    "For RAG evaluation, evaluate from the ragas library runs automated performance assessments using metrics like Faithfulness, AnswerRelevancy, and LLMContextPrecisionWithoutReference, which measure how accurate, relevant, and contextually grounded the model’s responses are.\n",
    "Finally, Dataset from the Hugging Face datasets library structures the inputs (questions, answers, and contexts) in a tabular format, and ChatOpenAI provides an interface for interacting with OpenAI’s chat-based LLMs within LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTY9GN4oWK3g"
   },
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import os                                                                       # Interact with the operating system (e.g., set environment variables)\n",
    "import json                                                                     # Read/write JSON data\n",
    "\n",
    "# Import libraries for working with PDFs and OpenAI\n",
    "from langchain.document_loaders import PyMuPDFLoader                            # Load and extract text from PDF files\n",
    "from openai import OpenAI                                                       # Access OpenAI's models and services\n",
    "\n",
    "# Import libraries for processing dataframes and text\n",
    "import tiktoken                                                                 # Tokenizer used for counting and splitting text for models\n",
    "import pandas as pd                                                             # Load, manipulate, and analyze tabular data\n",
    "\n",
    "# Import LangChain components for data loading, chunking, embedding, and vector DBs\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter              # Break text into overlapping chunks for processing\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings                        # Create vector embeddings using OpenAI's models  # type: ignore\n",
    "from langchain.vectorstores import Chroma                                       # Store and search vector embeddings using Chroma DB  # type: ignore\n",
    "\n",
    "# Import components to run evaluation on RAG pipeline outputs\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    Faithfulness,\n",
    "    AnswerRelevancy,\n",
    "    LLMContextPrecisionWithoutReference,\n",
    ")\n",
    "from datasets import Dataset                                                    # Used to structure the input (questions, answers, contexts etc.) in tabular format\n",
    "from langchain_openai import ChatOpenAI                                         # This is needed since LLM is used in metric computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtZWqj0wFTS1"
   },
   "source": [
    "## Question Answering using LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doLLV73YuVV6"
   },
   "source": [
    "Loading the google driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26974,
     "status": "ok",
     "timestamp": 1762623563770,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "7clY46DVua5x",
    "outputId": "88736096-857d-4afc-8c6c-922da54ec88d"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uq1lhM4WFTS2"
   },
   "source": [
    "#### Downloading and Loading the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EqvNFwzjh1u"
   },
   "source": [
    "This section loads API credentials from a configuration file and initializes the OpenAI client for further use.\n",
    "\n",
    "First, the JSON configuration file is opened and read. The json.load() function parses the file contents and converts them into a Python dictionary named config. From this dictionary, the OpenAI API key (OPENAI_API_KEY) and the API base URL (OPENAI_API_BASE) are extracted.\n",
    "\n",
    "Next, these credentials are stored as environment variables using the os.environ dictionary. This allows secure access to the API key and base URL throughout the notebook without hardcoding sensitive information in multiple places.\n",
    "\n",
    "Finally, the OpenAI client is initialized with the extracted API key and base URL. This establishes a connection to the OpenAI API, enabling subsequent operations such as generating embeddings, chat completions, or performing RAG-related tasks through the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dA3XQMWmQLJp"
   },
   "outputs": [],
   "source": [
    "# Load the JSON file and extract values\n",
    "file_name = '/content/drive/MyDrive/Colab_Notebooks/GenAI_course/Transformers_Text_Generation/Project_Medical_Assistant/config.json'                                                       # Name of the configuration file\n",
    "with open(file_name, 'r') as file:                                              # Open the config file in read mode\n",
    "    config = json.load(file)                                                    # Load the JSON content as a dictionary\n",
    "    OPENAI_API_KEY = config.get(\"OPENAI_API_KEY\")                                             # Extract the API key from the config\n",
    "    OPENAI_API_BASE = config.get(\"OPENAI_API_BASE\")                             # Extract the OpenAI base URL from the config\n",
    "\n",
    "# Store API credentials in environment variables\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY                                          # Set API key as environment variable\n",
    "os.environ[\"OPENAI_BASE_URL\"] = OPENAI_API_BASE                                 # Set API base URL as environment variable\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_API_BASE)                                                               # Create an instance of the OpenAI client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgCUBy59jnlX"
   },
   "source": [
    "This function, response(), is designed to generate text responses from an OpenAI language model based on a user-provided prompt.\n",
    "\n",
    "It takes in several parameters:\n",
    "\n",
    "1. user_prompt — the input text or query to which the model should respond.\n",
    "\n",
    "2. max_tokens — the maximum number of tokens (words or subwords) to include in the generated output.\n",
    "\n",
    "3. temperature — a parameter that controls randomness in the model’s responses. Lower values (e.g., 0.3) make the output more focused and deterministic.\n",
    "\n",
    "4. top_p — a parameter for nucleus sampling, controlling the diversity of possible words the model can choose from. A value of 0.95 allows for some variation while maintaining coherence.\n",
    "\n",
    "Inside the function, a chat completion request is created using the OpenAI client with the model gpt-4o-mini. The user’s input is passed as a message with the role \"user\". The API processes this request and returns a structured response containing multiple choices.\n",
    "\n",
    "Finally, the function returns only the textual content of the first generated message — completion.choices[0].message.content — which represents the model’s reply to the user prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VqlABrxFvV74"
   },
   "outputs": [],
   "source": [
    "# Define a function to get a response\n",
    "def response(user_prompt, max_tokens=500, temperature=0.3, top_p=0.95):\n",
    "    # Create a chat completion using the OpenAI client\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",                                                     # Specify the model to use\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": user_prompt}                            # User prompt is the input/query to respond to\n",
    "        ],\n",
    "        max_tokens=max_tokens,                                                  # Max number of tokens to generate in the response\n",
    "        temperature=temperature,                                                # Controls randomness in output\n",
    "        top_p=top_p                                                             # Controls diversity via nucleus sampling\n",
    "    )\n",
    "    return completion.choices[0].message.content                                # Return the text content from the model's reply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8YgK91SFjVY"
   },
   "source": [
    "### Question 1: What is the protocol for managing sepsis in a critical care unit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11014,
     "status": "ok",
     "timestamp": 1762627624370,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "-JLIVmpPQH0f",
    "outputId": "ecbf2edd-c8b2-4629-8fc5-43991a651df9"
   },
   "outputs": [],
   "source": [
    "question_1 = \"What is the protocol for managing sepsis in a critical care unit?\"\n",
    "base_response_question1 = response(question_1)\n",
    "print(base_response_question1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6yxICeVFjVc"
   },
   "source": [
    "### Question 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6095,
     "status": "ok",
     "timestamp": 1762627637826,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "BdiHRgEqQIP9",
    "outputId": "19e3ad95-20ba-471d-e6e6-c855001da65f"
   },
   "outputs": [],
   "source": [
    "question_2 = \"What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\"\n",
    "base_response_question2 = response(question_2)\n",
    "print(base_response_question2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oflaoOGiFjVd"
   },
   "source": [
    "### Question 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11436,
     "status": "ok",
     "timestamp": 1762627651277,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "N-mx9yboQIt-",
    "outputId": "aa5e33cb-32bd-43df-e1e2-0cf8960efa42"
   },
   "outputs": [],
   "source": [
    "question_3 = \"What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\"\n",
    "base_response_question3 = response(question_3)\n",
    "print(base_response_question3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUUqY4FbFjVe"
   },
   "source": [
    "### Question 4:  What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12423,
     "status": "ok",
     "timestamp": 1762627672503,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "TEsVMaKaQJzh",
    "outputId": "cce3f01f-ff34-441d-98fc-55fe98e8b24a"
   },
   "outputs": [],
   "source": [
    "question_4 = \"What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\"\n",
    "base_response_question4 = response(question_4)\n",
    "print(base_response_question4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8nziov9lLZQ"
   },
   "source": [
    "This section creates and displays a pandas DataFrame to organize the questions and their corresponding model-generated responses.\n",
    "\n",
    "The pd.DataFrame() constructor is used to build a structured table with two columns:\n",
    "\n",
    "1. \"questions\" — contains the list of question variables (question_1, question_2, question_3, question_4).\n",
    "\n",
    "2. \"base_prompt_responses\" — holds the responses generated by the model for each question (base_response_question1, base_response_question2, base_response_question3, base_response_question4).\n",
    "\n",
    "By aligning each question with its respective response, the DataFrame provides a clear, tabular view of the model’s outputs for easy inspection and comparison.\n",
    "\n",
    "Finally, result_df.head() displays the first few rows of the DataFrame, allowing a quick preview of the stored data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "executionInfo": {
     "elapsed": 73,
     "status": "ok",
     "timestamp": 1762627675529,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "aRlhwqLKygyR",
    "outputId": "dd3089ac-bf65-4f6b-ca8e-1c8a2ef48ff6"
   },
   "outputs": [],
   "source": [
    "# Create the DataFrame\n",
    "result_df = pd.DataFrame({\n",
    "    \"questions\": [question_1, question_2, question_3, question_4],\n",
    "    \"base_prompt_responses\": [base_response_question1, base_response_question2, base_response_question3, base_response_question4]})\n",
    "\n",
    "# Display the DataFrame\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5myZ5dOOefc"
   },
   "source": [
    "## Question Answering using LLM with Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I3dZrbnXeFqM"
   },
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "Here is a generalized prompt designed for an AI model that needs to retrieve information from the web.\n",
    "\n",
    "You are an AI assistant designed to be a helpful and accurate information resource. Your primary function is to search the web to find the most relevant and up-to-date information to answer user questions.\n",
    "\n",
    "User input will be a question or a request for information. The response should be within 500 words.\n",
    "\n",
    "When crafting your response:\n",
    "\n",
    "1. You must use a web search tool to find relevant information.\n",
    "\n",
    "2. Your answer should be accurate, concise, and directly address the user's query.\n",
    "\n",
    "3. You must cite your sources. For each key piece of information, provide the source title and URL where the information was found.\n",
    "\n",
    "Synthesize the information from one or more reliable sources into a coherent answer.\n",
    "\n",
    "1. If you cannot find a clear answer or the information is ambiguous, clearly state that.\n",
    "\n",
    "2. Do not provide personal opinions, speculations, or information that is not supported by the search results.\n",
    "\n",
    "3. Please adhere to the following response guidelines:\n",
    "\n",
    "4. Provide clear, direct answers based on the information retrieved from the web.\n",
    "\n",
    "5. Do not include information from outside the retrieved web search results.\n",
    "\n",
    "6. If the user's query is vague, you may ask for clarification.\n",
    "\n",
    "7. If a search yields no relevant information, clearly state that you were unable to find the answer.\n",
    "\n",
    "Here is an example of how to structure your response:\n",
    "\n",
    "Answer: [Provide the answer synthesized from the web search results.]\n",
    "\n",
    "Sources:\n",
    "\n",
    "[Information snippet 1] (Source: [Source Title], [URL])\n",
    "\n",
    "[Information snippet 2] (Source: [Source Title], [URL])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFjaJamVlSlG"
   },
   "source": [
    "This function, query_openai(), is designed to send a structured prompt and query to an OpenAI language model and return the generated response.\n",
    "\n",
    "It accepts two parameters:\n",
    "\n",
    "1. prompt — a system-level instruction that defines the model’s behavior or context (for example, specifying tone, role, or task).\n",
    "\n",
    "2. query — the user’s actual question or input that the model needs to answer.\n",
    "\n",
    "Inside the function, both are combined into a messages list following the chat completion format, where the \"system\" message sets context and the \"user\" message contains the actual query.\n",
    "\n",
    "The OpenAI client’s chat.completions.create() method is then called using the \"gpt-4o-mini\" model. Parameters like max_tokens, temperature, and top_p control the length, randomness, and diversity of the generated output.\n",
    "\n",
    "Finally, the function returns only the text portion of the model’s reply, extracted from response.choices[0].message.content. This modular design allows easy reuse for different prompts and queries, making it ideal for testing or building multi-turn conversational workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "msmz5tPBf4ho"
   },
   "outputs": [],
   "source": [
    "#prompt: Create a function that accepts a prompt and query, and returns the response generated by the OpenAI model.\n",
    "\n",
    "def query_openai(prompt, query):\n",
    "    \"\"\"\n",
    "    Queries the OpenAI model with a given prompt and query.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt for the model.\n",
    "        query (str): The query to be answered by the model.\n",
    "\n",
    "    Returns:\n",
    "        str: The model's response.\n",
    "    \"\"\"\n",
    "    #print (prompt)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # Or another suitable OpenAI model\n",
    "        messages=messages,\n",
    "        max_tokens=1000,  # Adjust max_tokens as needed\n",
    "        temperature=0.3,                                                # Controls randomness in output\n",
    "        top_p=0.95\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Jg3r_LWOeff"
   },
   "source": [
    "### Question 1: What is the protocol for managing sepsis in a critical care unit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "executionInfo": {
     "elapsed": 12397,
     "status": "ok",
     "timestamp": 1762623605327,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "YqM4VMw5ROhX",
    "outputId": "2a608e7e-6097-4ecd-8217-dd49e94178c5"
   },
   "outputs": [],
   "source": [
    "question_1 = \"What is the protocol for managing sepsis in a critical care unit?\"\n",
    "response_with_prompt_eng_1=query_openai(system_prompt,question_1)\n",
    "response_with_prompt_eng_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYpyw4HjOeff"
   },
   "source": [
    "### Question 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "executionInfo": {
     "elapsed": 8123,
     "status": "ok",
     "timestamp": 1762623617053,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "GXl09pFfRPBr",
    "outputId": "633f0b96-27ad-4c80-a81c-35ed1678e35b"
   },
   "outputs": [],
   "source": [
    "question_2 = \"What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\"\n",
    "response_with_prompt_eng_2=query_openai(system_prompt,question_2)\n",
    "response_with_prompt_eng_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRp92JQZOeff"
   },
   "source": [
    "### Question 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "executionInfo": {
     "elapsed": 19856,
     "status": "ok",
     "timestamp": 1762623644464,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "JOgATEpMRPve",
    "outputId": "eac967d6-dc14-4415-cb08-ef1238cb717a"
   },
   "outputs": [],
   "source": [
    "question_3 = \"What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\"\n",
    "response_with_prompt_eng_3=query_openai(system_prompt,question_3)\n",
    "response_with_prompt_eng_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AA45zwyUOefg"
   },
   "source": [
    "### Question 4:  What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "executionInfo": {
     "elapsed": 18461,
     "status": "ok",
     "timestamp": 1762623683286,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "VA7G8FOnRQZY",
    "outputId": "9e50245e-0d57-46dc-8f17-6978a5180517"
   },
   "outputs": [],
   "source": [
    "question_4 = \"What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\"\n",
    "response_with_prompt_eng_4=query_openai(system_prompt,question_4)\n",
    "response_with_prompt_eng_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGEMS0DTlyrK"
   },
   "source": [
    "This section appends a new column to the existing DataFrame to store model responses generated using prompt engineering techniques.\n",
    "\n",
    "A new column named responses_with_prompt_eng is created in result_df, containing responses (response_with_prompt_eng_1, response_with_prompt_eng_2, response_with_prompt_eng_3, response_with_prompt_eng_4) that correspond to each question already present in the DataFrame. This allows for a side-by-side comparison between base responses and improved responses generated after applying prompt engineering.\n",
    "\n",
    "Finally, result_df.head() displays the first few rows of the updated DataFrame, providing a quick preview of all questions and both sets of responses for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1762627717388,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "YASMVb2BzBj8",
    "outputId": "a92e6144-2186-4345-b671-7a7442bd8dda"
   },
   "outputs": [],
   "source": [
    "# Add the results to a new column in the DataFrame\n",
    "result_df['responses_with_prompt_eng'] = [response_with_prompt_eng_1, response_with_prompt_eng_2, response_with_prompt_eng_3, response_with_prompt_eng_4]\n",
    "\n",
    "# Display the DataFrame\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_O1PGdNO2M9"
   },
   "source": [
    "## Data Preparation for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTpWESc53dL9"
   },
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h67njGolpvOZ"
   },
   "source": [
    "This section loads a PDF document into memory for further text processing and analysis.\n",
    "\n",
    "The variable document_path stores the file path of the medical diagnosis manual PDF located in Google Drive. The PyMuPDFLoader from LangChain is then initialized with this path — it serves as a document loader specifically designed to read and extract text content from PDF files.\n",
    "\n",
    "By calling loader.load(), the PDF is parsed, and its textual content (along with metadata such as page information) is loaded into the document variable. This variable now holds the extracted text in a structured format, making it ready for subsequent steps such as chunking, embedding, or retrieval in a RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ybj2cEnzRSXq"
   },
   "outputs": [],
   "source": [
    "document_path = \"/content/drive/MyDrive/Colab_Notebooks/GenAI_course/Transformers_Text_Generation/Project_Medical_Assistant/medical_diagnosis_manual.pdf\"\n",
    "loader = PyMuPDFLoader(document_path)\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffj0ca3eZT4u"
   },
   "source": [
    "### Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZBT0A1vp0LV"
   },
   "source": [
    "This code iterates through and displays the text content of the first ten pages of the loaded PDF document.\n",
    "\n",
    "The for loop runs from 0 to 9 (a total of ten iterations), where each iteration represents one page of the document.\n",
    "Inside the loop:\n",
    "\n",
    "1. print(f\"Page Number : {i+1}\") displays the current page number, adjusted by +1 since Python indexing starts at 0.\n",
    "\n",
    "2. print(document[i].page_content) prints the actual text extracted from that page using the page_content attribute of the document object.\n",
    "\n",
    "The end=\"\\n\" argument ensures proper line breaks between pages, making the output easier to read.\n",
    "This step is primarily used for verifying that the PDF was loaded correctly and inspecting the extracted content before applying any text processing or chunking operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 176,
     "status": "ok",
     "timestamp": 1762623921982,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "-NuC-6SNRT7K",
    "outputId": "fc6cb65c-5dc7-4124-c2a9-b8037512c359"
   },
   "outputs": [],
   "source": [
    "# Display first 10 pages\n",
    "for i in range(10):\n",
    "    print(f\"Page Number : {i+1}\",end=\"\\n\")\n",
    "    print(document[i].page_content,end=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LECMxTH-zB-R"
   },
   "source": [
    "### Data Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50M1I0sYqH7E"
   },
   "source": [
    "This section initializes a text splitter to divide large text documents into smaller, manageable chunks that can be efficiently processed by language models.\n",
    "\n",
    "The RecursiveCharacterTextSplitter.from_tiktoken_encoder() method uses OpenAI’s tiktoken encoder (cl100k_base), which is compatible with modern LLMs such as GPT-4. This ensures that the chunking process aligns with how tokens are actually counted by the model.\n",
    "\n",
    "The chunk_size=256 parameter specifies that each chunk of text will contain up to 256 tokens (not characters). This helps maintain meaningful context within each chunk while preventing the model from exceeding its token limits during embedding or retrieval.\n",
    "\n",
    "By splitting text recursively at logical boundaries (e.g., paragraphs, sentences, or words), this splitter preserves context better than simple character-based splitting, improving both embedding quality and downstream RAG performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ir9Zi8rKRUmG"
   },
   "outputs": [],
   "source": [
    "# Initialize a text splitter that uses OpenAI's token encoder\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name='cl100k_base', # Encoding used by popular LLMs\n",
    "    chunk_size=256, # Each chunk will have up to 512 character\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90q5mQ0dmkYK"
   },
   "source": [
    "Split the document into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jY6an3-7mmpi"
   },
   "outputs": [],
   "source": [
    "document_chunks = loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIEwYJlpnFq7"
   },
   "source": [
    "Display the length of the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1762625724846,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "autkX8zYnIxG",
    "outputId": "e9c38695-f65d-4d2c-f248-feb6ee1c1657"
   },
   "outputs": [],
   "source": [
    "print(f\"Created {len(document_chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvHVejcWz0Bl"
   },
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZMjCa-wneDu"
   },
   "source": [
    "Initialize the OpenAI embedding model using API credentials, create embeddings for the first two chunks, and display the vector dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1851,
     "status": "ok",
     "timestamp": 1762625730336,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "Y43LeyZtnj92",
    "outputId": "707e1fa0-9630-493f-83b9-239e2995929f"
   },
   "outputs": [],
   "source": [
    "# Initialize the OpenAI Embeddings model with API credentials\n",
    "embedding_model = OpenAIEmbeddings(\n",
    "    openai_api_key=OPENAI_API_KEY,                                                     # Your OpenAI API key for authentication\n",
    "    openai_api_base=OPENAI_API_BASE                                             # The OpenAI API base URL endpoint\n",
    ")\n",
    "\n",
    "# Generate embeddings (vector representations) for the first two document chunks\n",
    "embedding_1 = embedding_model.embed_query(document_chunks[0].page_content)      # Embedding for chunk 0\n",
    "embedding_2 = embedding_model.embed_query(document_chunks[1].page_content)      # Embedding for chunk 1\n",
    "\n",
    "# Check and print the dimension (length) of the embedding vector\n",
    "print(\"Dimension of the embedding_1\", len(embedding_1))                   # Typically 1536 or 2048 depending on model\n",
    "print(\"Dimension of the embedding_2 \", len(embedding_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qiKCOv4X0d7B"
   },
   "source": [
    "### Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TyitdrxAohGU"
   },
   "source": [
    "Convert chunks into vector representations using the embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6Zdzw2CqWDb"
   },
   "source": [
    "This section defines and creates a directory to store the vector database that will hold text embeddings.\n",
    "\n",
    "The variable out_dir is assigned the name 'vectorstore', which serves as the folder where the vector database files (generated by tools like Chroma) will be saved.\n",
    "\n",
    "The if not os.path.exists(out_dir): condition checks whether a directory with that name already exists. If it does not, os.makedirs(out_dir) creates the directory.\n",
    "\n",
    "This ensures that the storage location for embeddings is properly set up before saving or initializing the vector store—preventing file path errors during later stages of the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q8px4FsLrFF0"
   },
   "outputs": [],
   "source": [
    "out_dir = 'vectorstore'    # complete the code to define the name of the vector database\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "  os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yH3h10CqUVf"
   },
   "source": [
    "This section builds a vector store—a database that stores vector embeddings of document chunks—and saves it for future retrieval operations.\n",
    "\n",
    "The Chroma.from_documents() method initializes a Chroma vector database by converting the provided text chunks (document_chunks) into embeddings using the specified embedding model (embedding_model). These embeddings represent the semantic meaning of the text, enabling efficient similarity searches later on.\n",
    "\n",
    "The parameter persist_directory=out_dir defines where the vector database files will be saved (in this case, the vectorstore folder). This ensures that the database can be reused across sessions without needing to regenerate embeddings each time.\n",
    "\n",
    "In summary, this code step transforms processed text into searchable vector representations and stores them locally, forming the foundation for semantic retrieval in the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQQVEkO-rNou"
   },
   "outputs": [],
   "source": [
    "# Building the vector store and saving it to disk for future use\n",
    "vectorstore = Chroma.from_documents(\n",
    "    document_chunks,                                                            # Documents to index\n",
    "    embedding_model,                                                            # Embedding model for converting text to vectors\n",
    "    persist_directory=out_dir                                                   # Save vector DB files here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NddlTdHTtd9m"
   },
   "source": [
    "Load Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1762626253318,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "Sj9kHfhhtggq",
    "outputId": "e7478419-cffe-4d2c-ebb2-888f6cea784e"
   },
   "outputs": [],
   "source": [
    "vectorstore = Chroma(\n",
    "    persist_directory=out_dir,\n",
    "    embedding_function=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEa5sKc41T1z"
   },
   "source": [
    "### Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9D5PhPTpc6A"
   },
   "source": [
    "Set up a similarity-based retriever to fetch the top 5 most relevant document chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wBlQUGx3RWUD"
   },
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type='similarity',                                                   # Use similarity search (based on vector distance)\n",
    "    search_kwargs={'k': 5}                                                      # Retrieve top 5 most relevant documents\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YoelW1cZuTuA"
   },
   "source": [
    "System and User Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TEJZ6XS4uiOo"
   },
   "outputs": [],
   "source": [
    "# Define the system prompt for the model\n",
    "qna_system_message = \"\"\"\n",
    "You are an AI assistant designed to support users in efficiently reviewing provided documents. Your task is to provide accurate, concise, and relevant answers based on the context provided from the source material.\n",
    "\n",
    "User input will include the necessary context for you to answer their questions. This context will begin with the token:\n",
    "\n",
    "###Context The context contains excerpts from one or more documents, along with associated metadata such as titles, authors, or specific sections relevant to the query.\n",
    "\n",
    "When crafting your response:\n",
    "\n",
    "1. Use only the provided context to answer the question.\n",
    "\n",
    "2. If the answer is found in the context, respond with concise and direct answers.\n",
    "\n",
    "3. Include the document title and, where applicable, a page or section reference as the source.\n",
    "\n",
    "4. If the question is unrelated to the context or the context is empty, clearly respond with: \"Sorry, this is out of my knowledge base.\"\n",
    "\n",
    "5. 6. Please adhere to the following response guidelines:\n",
    "\n",
    "Provide clear, direct answers using only the given context.\n",
    "\n",
    "1. Do not include any additional information outside of the context.\n",
    "\n",
    "2. Avoid rephrasing or generalizing unless explicitly relevant to the question.\n",
    "\n",
    "3. If no relevant answer exists in the context, respond with: \"Sorry, this is out of my knowledge base.\"\n",
    "\n",
    "4. If the context is not provided, your response should also be: \"Sorry, this is out of my knowledge base.\"\n",
    "\n",
    "Here is an example of how to structure your response:\n",
    "\n",
    "Answer: [Answer based on context]\n",
    "\n",
    "Source: [Source details with page or section]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0eofzuKu7V8"
   },
   "outputs": [],
   "source": [
    "# Define the user message template\n",
    "qna_user_message_template = \"\"\"\n",
    "###Context\n",
    "Here are some excerpts from GEN AI Research Paper and their sources that are relevant to the Gen AI question mentioned below:\n",
    "{context}\n",
    "\n",
    "###Question\n",
    "{question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkIteX4m6mny"
   },
   "source": [
    "### Response Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Y4Xn068qnAl"
   },
   "source": [
    "This function, generate_rag_response(), generates an AI-powered answer using a Retrieval-Augmented Generation (RAG) pipeline.\n",
    "\n",
    "It takes the user’s input (user_input) and retrieves the most relevant information from a document database before generating a final response with the OpenAI model.\n",
    "\n",
    "Here’s a step-by-step explanation:\n",
    "\n",
    "1. Retrieve relevant chunks:\n",
    "The retriever searches the vector database for the top k most relevant document chunks (retriever.get_relevant_documents(query=user_input, k=k)), where k determines how many context pieces to fetch.\n",
    "\n",
    "2. Prepare the context:\n",
    "The text content from each retrieved chunk is extracted and combined into a single string (context_for_query), which acts as the contextual background for the model.\n",
    "\n",
    "3. Format the user message:\n",
    "A user message template (qna_user_message_template) is updated by replacing placeholders {context} and {question} with the actual context and user query. This ensures the model receives both the question and supporting information.\n",
    "\n",
    "4. Generate the response:\n",
    "The OpenAI chat completion API (client.chat.completions.create) is called with:\n",
    "*   The system message (qna_system_message) defining the model’s role or behavior.\n",
    "*   The user message, containing the combined context and query. The parameters max_tokens, temperature, and top_p control response length, creativity, and sampling diversity.\n",
    "\n",
    "5. Return the model output:\n",
    "The text response is extracted from the API output (response.choices[0].message.content.strip()).\n",
    "If an error occurs during the API call, it catches the exception and returns a clear error message.\n",
    "\n",
    "Overall, this function ties together retrieval and generation—fetching factual context from stored documents and using an LLM to produce a coherent, context-aware answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NRny1MmIGAhI"
   },
   "outputs": [],
   "source": [
    "def generate_rag_response(user_input,k=5,max_tokens=500,temperature=0.3,top_p=0.95):\n",
    "    global qna_system_message,qna_user_message_template\n",
    "    # Retrieve relevant document chunks\n",
    "    relevant_document_chunks = retriever.get_relevant_documents(query=user_input,k=k)\n",
    "    context_list = [d.page_content for d in relevant_document_chunks]\n",
    "\n",
    "    # Combine document chunks into a single context\n",
    "    context_for_query = \". \".join(context_list)\n",
    "\n",
    "    user_message = qna_user_message_template.replace('{context}', context_for_query)\n",
    "    user_message = user_message.replace('{question}', user_input)\n",
    "\n",
    "    # Generate the response\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": qna_system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p\n",
    "        )\n",
    "        # Extract and print the generated text from the response\n",
    "        response = response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        response = f'Sorry, I encountered the following error: \\n {e}'\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffP1SRYbPQHN"
   },
   "source": [
    "## Question Answering using RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDhZWUYs0cy4"
   },
   "source": [
    "### Question 1: What is the protocol for managing sepsis in a critical care unit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "executionInfo": {
     "elapsed": 4693,
     "status": "ok",
     "timestamp": 1762626738622,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "d41WFi0e0cy5",
    "outputId": "7f0b2d22-cc62-4145-c38b-868af0309ec6"
   },
   "outputs": [],
   "source": [
    "question_1 = \"What is the protocol for managing sepsis in a critical care unit?\"\n",
    "response_with_rag_1 = generate_rag_response(question_1)\n",
    "response_with_rag_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WI3-E-a50cy6"
   },
   "source": [
    "### Question 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "executionInfo": {
     "elapsed": 4145,
     "status": "ok",
     "timestamp": 1762626891573,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "iQNK3Jzr0cy6",
    "outputId": "4d278a2e-00b6-4c66-d4ac-4d7508dd1bba"
   },
   "outputs": [],
   "source": [
    "question_2 = \"What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\"\n",
    "response_with_rag_2 = generate_rag_response(question_2)\n",
    "response_with_rag_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-u14Sks0cy6"
   },
   "source": [
    "### Question 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "executionInfo": {
     "elapsed": 3095,
     "status": "ok",
     "timestamp": 1762626902904,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "X5s0U10g0cy7",
    "outputId": "c31ae940-7ef9-40d0-ce2d-e87aa70f79af"
   },
   "outputs": [],
   "source": [
    "question_3 = \"What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\"\n",
    "response_with_rag_3 = generate_rag_response(question_3)\n",
    "response_with_rag_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nib7-0v0cy7"
   },
   "source": [
    "### Question 4:  What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "executionInfo": {
     "elapsed": 4041,
     "status": "ok",
     "timestamp": 1762626920125,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "gv-Fi9BL0cy7",
    "outputId": "92e9b84a-0f8e-427f-f024-00bfc9c2e138"
   },
   "outputs": [],
   "source": [
    "question_4 = \"What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\"\n",
    "response_with_rag_4 = generate_rag_response(question_4)\n",
    "response_with_rag_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esYeQFMAyASf"
   },
   "source": [
    "This section adds a new column to the existing DataFrame to store responses generated using the RAG (Retrieval-Augmented Generation) approach.\n",
    "\n",
    "A new column named responses_with_RAG is created in result_df, where each entry (response_with_rag_1, response_with_rag_2, response_with_rag_3, response_with_rag_4) corresponds to the model’s RAG-based answer for the respective question.\n",
    "\n",
    "This allows for direct comparison between:\n",
    "\n",
    "1. Base responses (without retrieval),\n",
    "\n",
    "2. Prompt-engineered responses, and\n",
    "\n",
    "3. RAG-enhanced responses.\n",
    "\n",
    "Finally, result_df.head() displays the first few rows of the updated DataFrame, providing a preview of all questions and their responses across different response-generation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "executionInfo": {
     "elapsed": 69,
     "status": "ok",
     "timestamp": 1762627737178,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "fF8oM8B9yDDT",
    "outputId": "c0e293de-8173-481f-fbce-bdf1646b2a08"
   },
   "outputs": [],
   "source": [
    "# Add the results to a new column in the DataFrame\n",
    "result_df['responses_with_RAG'] = [response_with_rag_1, response_with_rag_2, response_with_rag_3, response_with_rag_4]\n",
    "\n",
    "# Display the DataFrame\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRwURUk-15CT"
   },
   "source": [
    "## Output Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nB0a5rF-0Ygk"
   },
   "source": [
    "Defining required System Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cd411FQ10XQU"
   },
   "outputs": [],
   "source": [
    "groundedness_rater_system_message = \"\"\"\n",
    "You are tasked with rating AI generated answers to questions posed by users.\n",
    "You will be presented a question, context used by the AI system to generate the answer and an AI generated answer to the question.\n",
    "In the input, the question will begin with ###Question, the context will begin with ###Context while the AI generated answer will begin with ###Answer.\n",
    "\n",
    "Evaluation criteria:\n",
    "The task is to judge the extent to which the metric is followed by the answer.\n",
    "1 - The metric is not followed at all\n",
    "2 - The metric is followed only to a limited extent\n",
    "3 - The metric is followed to a good extent\n",
    "4 - The metric is followed mostly\n",
    "5 - The metric is followed completely\n",
    "\n",
    "Metric:\n",
    "The answer should be derived only from the information presented in the context\n",
    "\n",
    "Instructions:\n",
    "1. First write down the steps that are needed to evaluate the answer as per the metric.\n",
    "2. Give a step-by-step explanation if the answer adheres to the metric considering the question and context as the input.\n",
    "3. Next, evaluate the extent to which the metric is followed.\n",
    "4. Use the previous information to rate the answer using the evaluaton criteria and assign a score.\n",
    "\n",
    "Return only the Score in last in a dictionary format not json and score should be in the range of 1 to 5.\n",
    "Example {groundedness_score:4}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ccsl_ZVY0jMY"
   },
   "outputs": [],
   "source": [
    "relevance_rater_system_message = \"\"\"\n",
    "You are tasked with rating AI generated answers to questions posed by users.\n",
    "You will be presented a question, context used by the AI system to generate the answer and an AI generated answer to the question.\n",
    "In the input, the question will begin with ###Question, the context will begin with ###Context while the AI generated answer will begin with ###Answer.\n",
    "\n",
    "Evaluation criteria:\n",
    "The task is to judge the extent to which the metric is followed by the answer.\n",
    "1 - The metric is not followed at all\n",
    "2 - The metric is followed only to a limited extent\n",
    "3 - The metric is followed to a good extent\n",
    "4 - The metric is followed mostly\n",
    "5 - The metric is followed completely\n",
    "\n",
    "Metric:\n",
    "Relevance measures how well the answer addresses the main aspects of the question, based on the context.\n",
    "Consider whether all and only the important aspects are contained in the answer when evaluating relevance.\n",
    "\n",
    "Instructions:\n",
    "1. First write down the steps that are needed to evaluate the context as per the metric.\n",
    "2. Give a step-by-step explanation if the context adheres to the metric considering the question as the input.\n",
    "3. Next, evaluate the extent to which the metric is followed.\n",
    "4. Use the previous information to rate the context using the evaluaton criteria and assign a score.\n",
    "Return only the Score in last in a dictionary format not json and score should be in the range of 1 to 5.\n",
    "Example {relevance_score:4}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ikav3UNB0j3i"
   },
   "outputs": [],
   "source": [
    "user_message_template = \"\"\"\n",
    "###Question\n",
    "{question}\n",
    "\n",
    "###Context\n",
    "{context}\n",
    "\n",
    "###Answer\n",
    "{answer}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fkhosq_Nrro7"
   },
   "source": [
    "This function, generate_ground_relevance_response(), evaluates a model’s response based on two key metrics used in RAG evaluation — groundedness and relevance.\n",
    "\n",
    "It takes as input the user’s question (user_input) and the model’s generated answer (response), and returns two separate evaluations:\n",
    "one assessing how factually grounded the answer is in the retrieved documents, and another evaluating how relevant the answer is to the query.\n",
    "\n",
    "Here’s a step-by-step explanation:\n",
    "\n",
    "1. Retrieve supporting context:\n",
    "The retriever searches for the top 5 most relevant document chunks related to the user’s query. Their text content is stored in context_for_query.\n",
    "\n",
    "2. Construct evaluation prompts:\n",
    " Two separate prompts are built using formatted text blocks:\n",
    "\n",
    "*   groundedness_prompt — uses a predefined system instruction (groundedness_rater_system_message) asking the model to judge whether the provided answer is factually supported by the retrieved context.\n",
    "*   relevance_prompt — uses another instruction (relevance_rater_system_message) asking the model to rate how relevant the answer is to the user’s question.\n",
    "Both prompts include the context, question, and answer values dynamically inserted via the user_message_template.\n",
    "\n",
    "3. Generate model evaluations:\n",
    "The OpenAI model gpt-4o is called twice — once for groundedness and once for relevance.\n",
    "Parameters like max_tokens, temperature, and top_p control the output length and determinism (temperature is set to 0 for consistent evaluations).\n",
    "\n",
    "4. Return evaluation results:\n",
    "The function returns two text outputs — the groundedness evaluation (response_1) and the relevance evaluation (response_2) — extracted from the model’s responses.\n",
    "\n",
    "In summary, this function automates the self-evaluation step of a RAG pipeline by using an LLM to assess how well its answers align with retrieved evidence and how relevant they are to the user’s original query.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_nAUAI4O0n-N"
   },
   "outputs": [],
   "source": [
    "def generate_ground_relevance_response(user_input,response, max_tokens=500,temperature=0,top_p=0.95):\n",
    "    global qna_user_message_template\n",
    "\n",
    "    context_for_query = [doc.page_content for doc in retriever.get_relevant_documents(user_input, k=5)]\n",
    "\n",
    "    # Combine user_prompt and system_message to create the prompt\n",
    "    groundedness_prompt = f\"\"\"[INST]{groundedness_rater_system_message}\\n\n",
    "                {'user'}: {user_message_template.format(context=context_for_query, question=user_input, answer=response)}\n",
    "                [/INST]\"\"\"\n",
    "\n",
    "    # Combine user_prompt and system_message to create the prompt\n",
    "    relevance_prompt = f\"\"\"[INST]{relevance_rater_system_message}\\n\n",
    "                {'user'}: {user_message_template.format(context=context_for_query, question=user_input, answer=response)}\n",
    "                [/INST]\"\"\"\n",
    "\n",
    "    response_1 = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": groundedness_prompt}\n",
    "                ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p\n",
    "            )\n",
    "\n",
    "    response_2 = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": relevance_prompt}\n",
    "                ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p\n",
    "            )\n",
    "\n",
    "    return response_1.choices[0].message.content,response_2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_kKHCe2mqpE"
   },
   "source": [
    "#### **Evaluation 1: Base Prompt Response Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12234,
     "status": "ok",
     "timestamp": 1762628291173,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "3yiC32Kp1_Rl",
    "outputId": "287e9de4-d1f6-4f6e-b96a-3c433630c28c"
   },
   "outputs": [],
   "source": [
    "# Question 1\n",
    "llm_judge_base_ground_1,llm_judge_base_rel_1 = generate_ground_relevance_response(user_input=question_1, response=result_df.base_prompt_responses[0])\n",
    "print(llm_judge_base_ground_1,end=\"\\n\\n\")\n",
    "print(llm_judge_base_rel_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19977,
     "status": "ok",
     "timestamp": 1762628439235,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "hkUAx1qq1kjV",
    "outputId": "112733fe-1039-4c9d-8174-03f7df82bba8"
   },
   "outputs": [],
   "source": [
    "# Question 2\n",
    "llm_judge_base_ground_2,llm_judge_base_rel_2 = generate_ground_relevance_response(user_input=question_2, response=result_df.base_prompt_responses[1])\n",
    "print(llm_judge_base_ground_2,end=\"\\n\\n\")\n",
    "print(llm_judge_base_rel_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14022,
     "status": "ok",
     "timestamp": 1762628527406,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "tdb3WSe22A-g",
    "outputId": "7ed3c379-f0c5-40ba-c947-ca924ab645b1"
   },
   "outputs": [],
   "source": [
    "# Question 3\n",
    "llm_judge_base_ground_3,llm_judge_base_rel_3 = generate_ground_relevance_response(user_input=question_3, response=result_df.base_prompt_responses[2])\n",
    "print(llm_judge_base_ground_3,end=\"\\n\\n\")\n",
    "print(llm_judge_base_rel_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12971,
     "status": "ok",
     "timestamp": 1762628571018,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "5Oe4IOMg2Gh-",
    "outputId": "38e7a0d9-115c-4f40-f591-888cd7d75bc0"
   },
   "outputs": [],
   "source": [
    "# Question 4\n",
    "llm_judge_base_ground_4,llm_judge_base_rel_4 = generate_ground_relevance_response(user_input=question_4, response=result_df.base_prompt_responses[3])\n",
    "print(llm_judge_base_ground_4,end=\"\\n\\n\")\n",
    "print(llm_judge_base_rel_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdXnl6YGsGfu"
   },
   "source": [
    "This section adds two new columns to the existing DataFrame to store evaluation scores for the model’s base responses, focusing on groundedness and relevance.\n",
    "\n",
    "Each column stores the evaluation scores extracted from the outputs of the LLM-based evaluation functions:\n",
    "\n",
    "1. base_prompt_responses_groundedness_score — contains how factually supported (grounded) each base response is with respect to the retrieved context.\n",
    "\n",
    "2. base_prompt_responses_relevance_score — contains how relevant each base response is to the original user question.\n",
    "\n",
    "The notation [-2] indicates that the second-to-last element from each evaluation result (e.g., llm_judge_base_ground_1) holds the numeric score extracted from the model’s evaluation output.\n",
    "\n",
    "By adding these columns, the DataFrame now combines both responses and their quality scores, allowing an easy side-by-side comparison of each question, its answer, and the corresponding evaluation metrics.\n",
    "\n",
    "Finally, result_df.head() displays the first few rows, providing a quick preview of the dataset with these newly added evaluation columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1762628837192,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "F6hdLo532eWT",
    "outputId": "96f81d62-b248-4725-e40d-4a385409dafd"
   },
   "outputs": [],
   "source": [
    "result_df['base_prompt_responses_groundedness_score'] = [llm_judge_base_ground_1[-2], llm_judge_base_ground_2[-2], llm_judge_base_ground_3[-2],llm_judge_base_ground_4[-2]]\n",
    "result_df['base_prompt_responses_relevance_score'] = [llm_judge_base_rel_1[-2], llm_judge_base_rel_2[-2], llm_judge_base_rel_3[-2],llm_judge_base_rel_4[-2]]\n",
    "result_df\n",
    "\n",
    "# Display the DataFrame\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2J55TQZTnlYE"
   },
   "source": [
    "#### **Evaluation 2: Prompt Engineering Response Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14281,
     "status": "ok",
     "timestamp": 1762629271919,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "9J4uOcVz1_yL",
    "outputId": "429afe64-4838-440d-fc95-3be56d629bd4"
   },
   "outputs": [],
   "source": [
    "# Question 1\n",
    "ground1,rel1 = generate_ground_relevance_response(user_input=result_df.questions[0], response=result_df.responses_with_prompt_eng[0], max_tokens=516)\n",
    "print(ground1,end=\"\\n\\n\")\n",
    "print(rel1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11984,
     "status": "ok",
     "timestamp": 1762629287275,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "O0cnaHt44aj_",
    "outputId": "2602843c-9413-4ee8-801f-650747225938"
   },
   "outputs": [],
   "source": [
    "# Question 2\n",
    "ground2,rel2 = generate_ground_relevance_response(user_input=result_df.questions[1], response=result_df.responses_with_prompt_eng[1], max_tokens=516)\n",
    "print(ground2,end=\"\\n\\n\")\n",
    "print(rel2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13626,
     "status": "ok",
     "timestamp": 1762629303766,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "8UJs7tGt4bPy",
    "outputId": "9977cb45-5b1d-4143-ce04-1ee36cf7bf22"
   },
   "outputs": [],
   "source": [
    "# Question 3\n",
    "ground3,rel3 = generate_ground_relevance_response(user_input=result_df.questions[2], response=result_df.responses_with_prompt_eng[2], max_tokens=516)\n",
    "print(ground3,end=\"\\n\\n\")\n",
    "print(rel3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12742,
     "status": "ok",
     "timestamp": 1762629327417,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "6AcvcsWO4bnU",
    "outputId": "f38e3841-a5bd-417b-f5a4-a7aeedb7332a"
   },
   "outputs": [],
   "source": [
    "# Question 4\n",
    "ground4,rel4 = generate_ground_relevance_response(user_input=result_df.questions[3], response=result_df.responses_with_prompt_eng[3], max_tokens=516)\n",
    "print(ground4,end=\"\\n\\n\")\n",
    "print(rel4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYsWojP0sUlm"
   },
   "source": [
    "This section appends two additional columns to the DataFrame to record evaluation scores for responses generated using prompt engineering techniques.\n",
    "\n",
    "The columns capture how effectively prompt engineering improved the quality of model responses:\n",
    "\n",
    "1. prompt_engineering_responses_groundedness_score — represents how factually accurate or well-grounded each prompt-engineered response is based on retrieved context.\n",
    "\n",
    "2. prompt_engineering_responses_relevance_score — represents how relevant and contextually appropriate each response is to the corresponding question.\n",
    "\n",
    "The expressions like ground1[-2] and rel1[-2] extract the numeric evaluation scores (typically from the model’s returned evaluation output) for each respective response.\n",
    "\n",
    "By adding these two columns, the DataFrame now contains comparative evaluation metrics for both base responses and prompt-engineered responses, making it easier to analyze how prompt engineering impacts factual grounding and relevance.\n",
    "\n",
    "Finally, result_df.head() displays the first few rows of the updated DataFrame, showing the questions, responses, and their evaluation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1762629457456,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "fHH6aAIY5Wc3",
    "outputId": "06b9a693-3961-430b-f6c8-6188937c563a"
   },
   "outputs": [],
   "source": [
    "result_df['prompt_engineering_responses_groundedness_score'] = [ground1[-2], ground2[-2], ground3[-2],ground4[-2]]\n",
    "result_df['prompt_engineering_responses_relevance_score'] = [rel1[-2], rel2[-2], rel3[-2],rel4[-2]]\n",
    "result_df\n",
    "\n",
    "# Display the DataFrame\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7wXIFmfn4tX"
   },
   "source": [
    "#### **Evaluation 3: RAG Response Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8713,
     "status": "ok",
     "timestamp": 1762629605962,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "xYP3CcCx2AV0",
    "outputId": "ab78ed77-7830-47e4-9c12-d7176ec5dd11"
   },
   "outputs": [],
   "source": [
    "# Question 1\n",
    "llm_judge_rag_ground_1,llm_judge_rag_rel_1 = generate_ground_relevance_response(user_input=result_df.questions[0], response=result_df.responses_with_RAG[0], max_tokens=500)\n",
    "print(llm_judge_rag_ground_1,end=\"\\n\\n\")\n",
    "print(llm_judge_rag_rel_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7993,
     "status": "ok",
     "timestamp": 1762629730577,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "kifgrxUA6gAE",
    "outputId": "d7165551-cbc8-4a26-f0c6-895e5e804585"
   },
   "outputs": [],
   "source": [
    "# Question 2\n",
    "llm_judge_rag_ground_2,llm_judge_rag_rel_2 = generate_ground_relevance_response(user_input=result_df.questions[1], response=result_df.responses_with_RAG[1], max_tokens=500)\n",
    "print(llm_judge_rag_ground_2,end=\"\\n\\n\")\n",
    "print(llm_judge_rag_rel_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9526,
     "status": "ok",
     "timestamp": 1762629746504,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "VW31h5OI6pE8",
    "outputId": "e9bfff78-d9cc-4ead-807e-383c28713090"
   },
   "outputs": [],
   "source": [
    "# Question 3\n",
    "llm_judge_rag_ground_3,llm_judge_rag_rel_3 = generate_ground_relevance_response(user_input=result_df.questions[2], response=result_df.responses_with_RAG[2], max_tokens=500)\n",
    "print(llm_judge_rag_ground_2,end=\"\\n\\n\")\n",
    "print(llm_judge_rag_rel_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8535,
     "status": "ok",
     "timestamp": 1762629778976,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "9xGWz6o7643Q",
    "outputId": "dd9e9f97-933b-4dfd-e9cd-579662a4d872"
   },
   "outputs": [],
   "source": [
    "# Question 4\n",
    "llm_judge_rag_ground_4,llm_judge_rag_rel_4 = generate_ground_relevance_response(user_input=result_df.questions[3], response=result_df.responses_with_RAG[3], max_tokens=500)\n",
    "print(llm_judge_rag_ground_3,end=\"\\n\\n\")\n",
    "print(llm_judge_rag_rel_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kN0uUXWGskLK"
   },
   "source": [
    "This section adds two new columns to the DataFrame to store evaluation scores for responses generated using the RAG (Retrieval-Augmented Generation) approach.\n",
    "\n",
    "These columns capture how well the RAG-based responses perform in terms of factual correctness and contextual alignment:\n",
    "\n",
    "1. RAG_responses_groundedness_score — measures how factually accurate and evidence-backed each RAG-generated response is when compared with the retrieved document context.\n",
    "\n",
    "2. RAG_responses_relevance_score — measures how relevant and contextually appropriate each RAG-generated response is to the original user question.\n",
    "\n",
    "Each score (e.g., llm_judge_rag_ground_1[-2]) is extracted from the evaluation outputs returned by the LLM-based scoring process, where [-2] typically accesses the numeric score element.\n",
    "\n",
    "By adding these two columns, the DataFrame now includes a complete set of evaluation metrics — for base responses, prompt-engineered responses, and RAG responses — enabling detailed performance comparison across all response-generation methods.\n",
    "\n",
    "Finally, displaying result_df shows the fully updated table with questions, all response types, and their associated groundedness and relevance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1762629869137,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "lPQ8cMJe7FPp",
    "outputId": "61c66724-21dc-4bf6-cc4a-6ca5b0303519"
   },
   "outputs": [],
   "source": [
    "result_df['RAG_responses_groundedness_score'] = [llm_judge_rag_ground_1[-2], llm_judge_rag_ground_2[-2], llm_judge_rag_ground_3[-2],llm_judge_rag_ground_4[-2]]\n",
    "result_df['RAG_responses_relevance_score'] = [llm_judge_rag_rel_1[-2], llm_judge_rag_rel_2[-2], llm_judge_rag_rel_3[-2],llm_judge_rag_rel_4[-2]]\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdu7P6BqsoUV"
   },
   "source": [
    "This section standardizes and analyzes the evaluation metrics stored in the DataFrame by cleaning column names, ensuring numeric data types, and calculating average scores for each response generation approach.\n",
    "\n",
    "Here’s what happens step by step:\n",
    "\n",
    "1. Clean column names:\n",
    "result_df.columns = result_df.columns.str.strip() removes any leading or trailing spaces from the column names to prevent reference errors caused by inconsistent naming.\n",
    "\n",
    "2. Select relevant columns:\n",
    "The list cols defines the six numeric evaluation columns corresponding to groundedness and relevance scores for the three approaches — Base Prompt, Prompt Engineering, and RAG.\n",
    "\n",
    "3. Convert columns to numeric:\n",
    "result_df[cols].apply(pd.to_numeric, errors='coerce') ensures that all selected columns are treated as numeric values. Any non-numeric entries (e.g., text or None) are safely converted to NaN using errors='coerce'.\n",
    "\n",
    "4. Compute averages:\n",
    "The mean(numeric_only=True) function calculates the average groundedness and relevance scores for each method:\n",
    "\n",
    "\n",
    "*   Base Prompt Evaluation — measures the model’s raw performance without enhancements.\n",
    "*   Prompt Engineering Evaluation — reflects how tailored prompts improve factual grounding and relevance.\n",
    "*  RAG Response Evaluation — evaluates how integrating retrieval-based context impacts accuracy and contextual alignment.\n",
    "\n",
    "5. Print summary results:\n",
    "The printed output provides a concise comparison of the average groundedness and relevance scores across the three approaches, highlighting which method delivers the best overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1762630320000,
     "user": {
      "displayName": "Rohan Kulkarni",
      "userId": "11589941396190831510"
     },
     "user_tz": 360
    },
    "id": "Jh-O2nSm7hzO",
    "outputId": "7c6e6f6e-9490-43e9-8a6f-fefa827c4163"
   },
   "outputs": [],
   "source": [
    "result_df.columns = result_df.columns.str.strip()\n",
    "cols = [\n",
    "    'base_prompt_responses_groundedness_score',\n",
    "    'base_prompt_responses_relevance_score',\n",
    "    'prompt_engineering_responses_groundedness_score',\n",
    "    'prompt_engineering_responses_relevance_score',\n",
    "    'RAG_responses_groundedness_score',\n",
    "    'RAG_responses_relevance_score'\n",
    "]\n",
    "result_df[cols] = result_df[cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "print(\"Average scores for Base Prompt Evaluation:\")\n",
    "print(result_df[['base_prompt_responses_groundedness_score', 'base_prompt_responses_relevance_score']].mean(numeric_only=True))\n",
    "print(\"\\n\")\n",
    "print(\"Average scores for Prompt Engineering Evaluation:\")\n",
    "print(result_df[['prompt_engineering_responses_groundedness_score', 'prompt_engineering_responses_relevance_score']].mean(numeric_only=True))\n",
    "print(\"\\n\")\n",
    "print(\"\\nAverage scores for RAG Response Evaluation:\")\n",
    "print(result_df[['RAG_responses_groundedness_score', 'RAG_responses_relevance_score']].mean(numeric_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7QICRU-njdj"
   },
   "source": [
    "## Actionable Insights and Business Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aT73ezEN-DUP"
   },
   "source": [
    "1. **Actionable Insights**\n",
    "\n",
    "* RAG Significantly Outperforms Other Methods:\n",
    "Integrating retrieval mechanisms (vector database + embeddings) provides both high factual grounding and strong contextual relevance. This confirms that augmenting the model with real-world knowledge sources is the most reliable approach for domain-specific or information-sensitive use cases (e.g., healthcare, finance, customer support).\n",
    "\n",
    "* Prompt Engineering Alone Is Insufficient:\n",
    "Despite structured instructions, prompt engineering without external knowledge leads to lower factual accuracy. It highlights the model’s limitation in reasoning accurately without grounding its responses in verified information.\n",
    "\n",
    "* Base Prompt Offers Good Relevance but Moderate Accuracy:\n",
    "The model’s baseline performance suggests it understands the context but occasionally “hallucinates” details—typical of LLMs when not connected to a factual retrieval layer.\n",
    "\n",
    "2. **Business Recommendations**\n",
    "\n",
    "- Adopt RAG-Based Systems for Production:\n",
    "    - Implement RAG pipelines for all use cases requiring factual reliability (e.g., medical advice, policy guidance, financial documentation).\n",
    "\n",
    "    - Store relevant domain data in a well-structured vector database (e.g., Chroma, Pinecone) to ensure accurate retrieval.\n",
    "\n",
    "- Use Prompt Engineering as a Complementary Strategy:\n",
    "\n",
    "    - Continue refining prompts to improve tone, structure, and clarity—but always pair them with retrieval for factual consistency.\n",
    "\n",
    "    - Use prompt engineering for conversational quality and contextual framing, while RAG ensures correctness.\n",
    "\n",
    "- Establish Continuous Evaluation Frameworks:\n",
    "\n",
    "    - Regularly monitor groundedness and relevance scores to detect performance drift.\n",
    "\n",
    "    - Automate this evaluation pipeline (using libraries like ragas) to ensure quality consistency as data or models evolve.\n",
    "\n",
    "- Leverage Insights for Business Scaling:\n",
    "\n",
    "    - Deploy RAG-enhanced models in customer-facing products to improve trust and reduce misinformation.\n",
    "\n",
    "    - Extend the same architecture to other internal knowledge domains (e.g., HR FAQs, product documentation) to increase productivity.\n",
    "\n",
    "3. **Summary**: RAG-based AI systems deliver the most accurate, reliable, and contextually aligned results. For business adoption, combining retrieval mechanisms with well-engineered prompts offers the best balance between natural interaction and factual precision — driving both customer trust and operational efficiency."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "CARPKFwm6Bz4",
    "by9EvAnkSpZf"
   ],
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1WlkSMM3TGFoWzZEQHhqe_8k573o6y05Q",
     "timestamp": 1753443200170
    }
   ]
  },
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
